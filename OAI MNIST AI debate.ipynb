{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjXDUdojRORb"
      },
      "source": [
        "# AI safety via debate\n",
        "OAI article: https://openai.com/index/debate/\n",
        "\n",
        "Paper: https://arxiv.org/pdf/1805.00899\n",
        "\n",
        "Implementation by Alex Chen\n",
        "\n",
        "Jul 2024"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDwV8qMBQzce"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "-p8AER-rQhaV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import random\n",
        "from torchvision.transforms import ToTensor\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAXdR4RGQzCE"
      },
      "source": [
        "# Training the Judge\n",
        "The judge is a classifier trained to \"predict MNIST digits from 6 non-black digits, sampled at random for each presentation when pretraining.\"\n",
        "\n",
        "As per the paper, their original sparse classifier achieved 59.4% accuracy. They use a convolutional neural net."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Pw-A0C8PT_aO"
      },
      "outputs": [],
      "source": [
        "n_epochs = 3\n",
        "batch_size_train = 64\n",
        "batch_size_test = 1000\n",
        "learning_rate = 0.0005\n",
        "momentum = 0.5\n",
        "log_interval = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVggtk7Oe77V",
        "outputId": "5f78934d-a849-4947-ae52-850337151bd2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.654951\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.112054\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.798163\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.829594\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 1.591716\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 1.783678\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 1.771029\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 1.430417\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 1.712763\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 1.590702\n",
            "\n",
            "Test set: Average loss: 1.4883, Accuracy: 4791/10000 (48%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 1.440902\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 1.430490\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 1.537496\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 1.650518\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 1.371253\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 1.337301\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 1.454696\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 1.538943\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 1.446250\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 1.441667\n",
            "\n",
            "Test set: Average loss: 1.4312, Accuracy: 4952/10000 (50%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 1.312153\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 1.586529\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 1.424207\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 1.405046\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 1.399467\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 1.382309\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 1.462464\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 1.454110\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 1.564889\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 1.443672\n",
            "\n",
            "Test set: Average loss: 1.4195, Accuracy: 4961/10000 (50%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 1.277502\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 1.483839\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 1.504731\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 1.676307\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 1.470169\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 1.212281\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 1.434710\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 1.450702\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 1.481758\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 1.441824\n",
            "\n",
            "Test set: Average loss: 1.4124, Accuracy: 5007/10000 (50%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 1.382888\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 1.448823\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 1.535118\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 1.674076\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 1.692200\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 1.554963\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 1.340391\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 1.383336\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 1.408097\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 1.435921\n",
            "\n",
            "Test set: Average loss: 1.3999, Accuracy: 5079/10000 (51%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 1.703044\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 1.652512\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 1.444550\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 1.571924\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 1.557438\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 1.755984\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 1.194607\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 1.331372\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 1.212403\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 1.424833\n",
            "\n",
            "Test set: Average loss: 1.3739, Accuracy: 5142/10000 (51%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 1.882323\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 1.224746\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 1.218556\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 1.649076\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 1.451324\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 1.354857\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 1.280946\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 1.500385\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 1.370165\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 1.582253\n",
            "\n",
            "Test set: Average loss: 1.3683, Accuracy: 5172/10000 (52%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 1.297178\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 1.324090\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 1.404009\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 1.415379\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 1.420894\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 1.601256\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 1.298604\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 1.531814\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 1.690714\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 1.189334\n",
            "\n",
            "Test set: Average loss: 1.3696, Accuracy: 5124/10000 (51%)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "class RandomPixelSelection(object):\n",
        "    def __init__(self, num_pixels=6):\n",
        "        self.num_pixels = num_pixels\n",
        "\n",
        "    def __call__(self, img):\n",
        "        img_np = np.array(img)\n",
        "        flattened = img_np.flatten()\n",
        "        non_black_indices = np.where(flattened > 0)[0]\n",
        "\n",
        "        if len(non_black_indices) < self.num_pixels:\n",
        "            selected_indices = non_black_indices\n",
        "        else:\n",
        "            selected_indices = np.random.choice(non_black_indices, self.num_pixels, replace=False)\n",
        "\n",
        "        new_img = np.zeros_like(flattened)\n",
        "        new_img[selected_indices] = flattened[selected_indices]\n",
        "        new_img = new_img.reshape(img_np.shape)\n",
        "\n",
        "        new_img = torch.tensor(new_img, dtype=torch.float32)\n",
        "        new_img = (new_img - 0.1307) / 0.3081\n",
        "\n",
        "        return new_img.unsqueeze(0)\n",
        "\n",
        "transform = torchvision.transforms.Compose([\n",
        "    RandomPixelSelection(num_pixels=6)\n",
        "])\n",
        "\n",
        "batch_size_train = 64\n",
        "batch_size_test = 1000\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    torchvision.datasets.MNIST('~/mnist_data/', train=True, download=True, transform=transform),\n",
        "    batch_size=batch_size_train, shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    torchvision.datasets.MNIST('~/mnist_data/', train=False, download=True, transform=transform),\n",
        "    batch_size=batch_size_test, shuffle=True)\n",
        "\n",
        "class Judge(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Judge, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(6, 12, kernel_size=5)\n",
        "        self.fc1 = nn.Linear(12 * 20 * 20, 60)\n",
        "        self.fc2 = nn.Linear(60, 30)\n",
        "        self.fc3 = nn.Linear(30, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = x.view(-1, 12 * 20 * 20)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = Judge().to(device)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.7)\n",
        "\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} '\n",
        "                  f'({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} '\n",
        "          f'({100. * correct / len(test_loader.dataset):.0f}%)\\n')\n",
        "\n",
        "n_epochs = 8\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    test(model, device, test_loader)\n",
        "    scheduler.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "example_data, example_target = next(iter(test_loader))\n",
        "example_data, example_target = example_data.to(device), example_target.to(device)\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    output = model(example_data)\n",
        "    pred = output.argmax(dim=1, keepdim=True)\n",
        "    for i in range(len(pred)):\n",
        "        print(f'Predicted: {pred[i].item()}, Actual: {example_target[i].item()}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "class StandardMNIST(object):\n",
        "    def __call__(self, img):\n",
        "        img_np = np.array(img)\n",
        "        new_img = torch.tensor(img_np, dtype=torch.float32)\n",
        "        return new_img.unsqueeze(0)\n",
        "\n",
        "standardTransform = torchvision.transforms.Compose([\n",
        "    StandardMNIST(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size_test = 1\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    torchvision.datasets.MNIST('~/mnist_data/', train=False, download=True, transform=standardTransform),\n",
        "    batch_size=batch_size_test, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([6])\n"
          ]
        }
      ],
      "source": [
        "example_image, example_label = next(iter(test_loader))\n",
        "example_image_dim = example_image[0][0]\n",
        "print(example_label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# The Agent\n",
        "\n",
        "OAI paper uses pure Monte Carlo Tree Search with 10k rollouts per move\n",
        "\n",
        "The selection criteria is the PUCT variant: the win rate + Upper bound\n",
        "\n",
        "tree layers alternate between the truthful agent and the lying agent\n",
        "\n",
        "each node can track the:\n",
        "- parent\n",
        "- children\n",
        "- full game state up to that node, which also reveals which agent they are and what turn it is\n",
        "- value; number of wins\n",
        "\n",
        "\n",
        "the root of the tree is the empty state\n",
        "\n",
        "first move is truthful agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Node():\n",
        "    def __init__(self, parent=None, children={}, winRate=0, timesVisited=0, depth=0):\n",
        "        self.parent = parent\n",
        "        self.children = children\n",
        "        self.timesVisited = timesVisited\n",
        "        self.winRate = [] # [wins, total visits]\n",
        "        self.depth = depth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MCTS():\n",
        "    def __init__(self, image, targetLabel, model):\n",
        "        self.root = Node(parent=None)\n",
        "        self.visitedLeaves = {} # may not need due to sparsity\n",
        "        self.image = image\n",
        "        self.targetLabel = targetLabel\n",
        "        self.model = model\n",
        "        self.non_black_pixels = []\n",
        "        print(self.image.shape)\n",
        "        for i in range(self.image.shape[0]):\n",
        "            for j in range(self.image.shape[1]):\n",
        "                if self.image[i, j] > -4.23e-01: # -0.42421... is what zeroes get mapped to\n",
        "                    self.non_black_pixels.append((i, j))\n",
        "        \n",
        "        self.numValidActions = len(self.non_black_pixels)\n",
        "    \n",
        "    def calculateUtility(self, winRate, sumVisited, numVisits):\n",
        "        return winRate + (1 / self.numValidActions) * (math.sqrt(1 + sumVisited)) / (1 + numVisits)\n",
        "\n",
        "    def chooseNextNode(self, curNode, chosen):\n",
        "        sumVisited = 0\n",
        "        for child in curNode.children:\n",
        "            sumVisited += child.timesVisited\n",
        "        \n",
        "        maxUtility = -1\n",
        "        maxChildIndices = []\n",
        "\n",
        "        for i in range(self.numValidActions):\n",
        "            if i not in chosen:\n",
        "                if curNode.children[i] is not None:\n",
        "                    util = self.calculateUtility(curNode.children[i].winRate, sumVisited, curNode.children[i].timesVisited)\n",
        "                else:\n",
        "                    util = self.calculateUtility(0, sumVisited, curNode.children[i].timesVisited)\n",
        "\n",
        "                if util > maxUtility:\n",
        "                    maxChildIndices = [i]\n",
        "                    maxUtility = util\n",
        "                elif util == maxUtility:\n",
        "                    maxChildIndices.append(i)\n",
        "        \n",
        "        maxChildIndex = random.choice(maxChildIndices)\n",
        "        if curNode.children[maxChildIndex] is not None:\n",
        "            curNode.children[maxChildIndex].timesVisited += 1\n",
        "        else:\n",
        "            curNode.children[maxChildIndex] = Node(parent=curNode, winRate=0, timesVisited=1)\n",
        "\n",
        "        return curNode.children[maxChildIndex], maxChildIndex\n",
        "\n",
        "\n",
        "    def backprop(self, leaf, truthWin):\n",
        "        curNode = leaf\n",
        "        while curNode.parent is not None:\n",
        "            if truthWin and curNode.depth % 2 == 1:\n",
        "                curNode.winRate += 1\n",
        "            elif not truthWin and curNode.depth % 2 == 0:\n",
        "                curNode.winRate += 1\n",
        "            curNode = curNode.parent\n",
        "    \n",
        "\n",
        "    def judge(self, chosen):\n",
        "        judge_image = torch.zeros((28, 28))\n",
        "        chosen_pixels = [self.non_black_pixels[index] for index in chosen]\n",
        "        for c in chosen_pixels:\n",
        "            judge_image[c[0]][c[1]] = self.image[c[0]][c[1]]\n",
        "        judge_image = judge_image.unsqueeze(0).unsqueeze(0) # this is a batch consisting of a single image lol\n",
        "        output = self.model(judge_image)\n",
        "        pred = output.argmax(dim=1, keepdim=True).item()\n",
        "        return pred == self.targetLabel.item()\n",
        "    \n",
        "        \n",
        "    def rollouts(self, numRollouts):\n",
        "        for i in range(numRollouts):\n",
        "            curNode = self.root\n",
        "            chosen = []\n",
        "            while curNode.depth != 6:\n",
        "                nextNode, chosenIndex = self.chooseNextNode(curNode, chosen)\n",
        "                chosen.append(chosenIndex)\n",
        "                curNode = nextNode\n",
        "            \n",
        "            truthWin = self.judge(chosen)\n",
        "            self.backprop(curNode, truthWin)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([28, 28])\n",
            "torch.Size([28, 28])\n",
            "tensor([6])\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "print(example_image_dim.shape)\n",
        "a = MCTS(image=example_image_dim, targetLabel=example_label, model=model)\n",
        "print(example_label)\n",
        "chosen = [10,1,3,75,4,155]\n",
        "print(a.judge(chosen))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "IDwV8qMBQzce"
      ],
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
